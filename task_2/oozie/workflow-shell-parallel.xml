<workflow-app xmlns='uri:oozie:workflow:0.1' name='${workflow_name}'>
    <start to="forking"/>
     <fork name="forking">
      <path start="shell-process_1"/>
      <path start="shell-process_2"/>
      <path start="shell-process_3"/>
      <path start="shell-process_4"/>
      <path start="shell-process_5"/>
      <path start="shell-process_6"/>
    </fork>
    <action name='shell-process_1'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_DIM_CATEGORY.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_DIM_CATEGORY.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
    <action name='shell-process_2'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_DIM_CUSTOMERS.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_DIM_CUSTOMERS.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
    <action name='shell-process_3'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_DIM_EVENT_TYPE.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_DIM_EVENT_TYPE.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
   <action name='shell-process_4'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_DIM_PRODUCTS.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_DIM_PRODUCTS.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
    <action name='shell-process_5'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_DIM_SUPPLIERS.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_DIM_SUPPLIERS.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
    <action name='shell-process_6'>
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>spark-submit</exec>
            <argument>--master</argument>
            <argument>local[*]</argument>
            <argument>--conf</argument>
            <argument>spark.driver.extraJavaOptions=-Diop.version=4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.executor.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.am.extraJavaOptions=-Diop.version = 4.1.0.0</argument>
            <argument>Producer_FCT_PROD.py</argument>
            <env-var>SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark</env-var>
            <file>${PythonPath}/Producer_FCT_PROD.py</file>
            <capture-output/>
        </shell>
        <ok to="joining" />
        <error to="kill" />
    </action>
    <join name="joining" to="end"/>
    <kill name="kill">
        <message>Workflow failed, error
            message[${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>
    <end name='end' />
</workflow-app>