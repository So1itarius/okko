examplesRoot=spark_example_root


workflow_name="any_name"
nameNode=hdfs://hadoop
jobTracker=hadoop.host.ru:port
master=yarn/local[*]
mode=cluster/client
child_app="child_app_name"
queueName=any_name
PythonPath=${nameNode}/.../${examplesRoot}/python-files
input-data="${nameNode}/.../${examplesRoot}/input-data"
output-data="${nameNode}/.../${examplesRoot}/output-data"
workflowPath=${nameNode}/.../${examplesRoot}
arg_1=hdfs://cdh631.itfbgroup.local:8020/user/usertest/okko/oozie/Cons/python-files/offset_value_dim_customers.txt
arg_2=hdfs://cdh631.itfbgroup.local:8020/user/usertest/okko/oozie/Cons/python-files/offset_value_dim_products.txt
arg_3=hdfs://cdh631.itfbgroup.local:8020/user/usertest/okko/oozie/Cons/python-files/offset_value_dim_suppliers.txt
arg_4=hdfs://cdh631.itfbgroup.local:8020/user/usertest/okko/oozie/Cons/python-files/offset_value_fct_events.txt

startTime=yyyy-MM-dd'T'HH:mm'Z'
endTime=yyyy-MM-dd'T'HH:mm'Z'
step=int
coordinator_name="any_name"
timezone=zone
oozie.coord.application.path=${nameNode}/.../${examplesRoot}

oozie.use.system.libpath=true/false
